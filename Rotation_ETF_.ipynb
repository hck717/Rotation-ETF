{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0IiYDcLH3aEoQK32nXJw9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hck717/Rotation-ETF/blob/main/Rotation_ETF_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas fredapi tweepy vaderSentiment alpha_vantage pytrends"
      ],
      "metadata": {
        "id": "bhoTZpvyndR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ta-lib-bin > log.txt"
      ],
      "metadata": {
        "id": "FotifmhtqqLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance==0.2.54 pandas_datareader fredapi pytrends pandas numpy requests beautifulsoup4"
      ],
      "metadata": {
        "id": "VB3C4pLELpBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Collection"
      ],
      "metadata": {
        "id": "ynpLk-t05CyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FRED_API_KEY = os.getenv(\"FRED_API_KEY\", \"2e978cd1b8ac041452388ed44ac1dcd6\")"
      ],
      "metadata": {
        "id": "GheYQ1c0L4Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import pandas_datareader as pdr\n",
        "from fredapi import Fred\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Define ETF universe\n",
        "ETF_UNIVERSE = [\n",
        "    \"MTUM\", \"VLUE\", \"QUAL\", \"USMV\",  # U.S. Factor ETFs\n",
        "    \"XLK\", \"XLV\", \"XLE\", \"XLU\",      # U.S. Sectors\n",
        "    \"EEM\", \"EFA\",                    # Global\n",
        "    \"TLT\", \"IEF\", \"TIP\",             # Fixed Income\n",
        "    \"BIL\"                            # Cash Proxy\n",
        "]\n",
        "\n",
        "# Cross-asset tickers\n",
        "CROSS_ASSETS = [\"SPY\", \"TLT\", \"GLD\"]\n",
        "\n",
        "# Date range for data collection\n",
        "START_DATE = \"2010-01-01\"\n",
        "END_DATE = datetime.today().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# Directory to store CSV files\n",
        "DATA_DIR = \"etf_data\"\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "# Initialize FRED API with your provided API key\n",
        "fred = Fred(api_key=\"2e978cd1b8ac041452388ed44ac1dcd6\")\n",
        "\n",
        "# Toggle for Google Trends usage (set to False to disable for now)\n",
        "use_google_trends = False  # Change to True to enable Google Trends data collection\n",
        "\n",
        "# Helper function to save DataFrame to CSV\n",
        "def save_to_csv(df, file_name):\n",
        "    file_path = os.path.join(DATA_DIR, f\"{file_name}.csv\")\n",
        "    df.to_csv(file_path)\n",
        "    print(f\"Saved {file_name} to {file_path}\")\n",
        "\n",
        "# 1. Price and Volume Data (yfinance)\n",
        "def collect_price_volume_data(tickers, retries=3):\n",
        "    price_data = {}\n",
        "    volume_data = {}\n",
        "    avg_daily_volume = {}\n",
        "    failed_tickers = []\n",
        "    missing_data_tickers = []\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"Collecting price/volume data for {ticker}\")\n",
        "        attempt = 0\n",
        "        success = False\n",
        "        while attempt < retries and not success:\n",
        "            try:\n",
        "                data = yf.download(ticker, start=START_DATE, end=END_DATE, auto_adjust=False, progress=False)\n",
        "                if data.empty or 'Adj Close' not in data.columns:\n",
        "                    raise ValueError(f\"No data or 'Adj Close' missing for {ticker}\")\n",
        "\n",
        "                # Check for missing data in early dates\n",
        "                if data.iloc[0:].isna().all().all():\n",
        "                    missing_data_tickers.append(ticker)\n",
        "                else:\n",
        "                    price_data[ticker] = data[[\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\"]]\n",
        "                    volume_data[ticker] = data[\"Volume\"]\n",
        "                    recent_volume = data[\"Volume\"][-30:]\n",
        "                    avg_daily_volume[ticker] = recent_volume.mean() if not recent_volume.empty else np.nan\n",
        "                    success = True\n",
        "            except Exception as e:\n",
        "                print(f\"Error collecting data for {ticker} (attempt {attempt + 1}/{retries}): {e}\")\n",
        "                attempt += 1\n",
        "                time.sleep(2 ** attempt)\n",
        "        if not success:\n",
        "            print(f\"Failed to collect data for {ticker} after {retries} attempts.  Skipping.\")\n",
        "            failed_tickers.append(ticker)\n",
        "\n",
        "    if not price_data:\n",
        "        print(\"Warning: No price data collected for any ticker.\")\n",
        "        return pd.DataFrame(), pd.DataFrame(), pd.Series(), failed_tickers, missing_data_tickers\n",
        "\n",
        "    price_df = pd.concat(price_data, axis=1)\n",
        "    volume_df = pd.concat(volume_data, axis=1)\n",
        "    avg_volume_df = pd.Series(avg_daily_volume, name=\"Avg_Daily_Volume\")\n",
        "\n",
        "    save_to_csv(price_df, \"price_data\")\n",
        "    save_to_csv(volume_df, \"volume_data\")\n",
        "    save_to_csv(avg_volume_df, \"avg_daily_volume\")\n",
        "\n",
        "    return price_df, volume_df, avg_volume_df, failed_tickers, missing_data_tickers\n",
        "\n",
        "# 2. Factor-Specific Data\n",
        "def collect_factor_specific_data(price_df, successful_tickers):\n",
        "    factor_data = {}\n",
        "\n",
        "    if price_df.empty:\n",
        "        print(\"No price data available for factor-specific calculations.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for ticker in successful_tickers:\n",
        "        print(f\"Calculating factor-specific data for {ticker}\")\n",
        "        try:\n",
        "            ticker_data = price_df[ticker][\"Adj Close\"]\n",
        "            ticker_data = ticker_data.dropna()\n",
        "\n",
        "            if len(ticker_data) < 252:  # Need at least 252 days for 12-month momentum\n",
        "                print(f\"Skipping {ticker}: Insufficient data for momentum calculation.\")\n",
        "                continue\n",
        "\n",
        "            # Momentum: 12-month return\n",
        "            momentum = ticker_data.pct_change(periods=252).dropna()\n",
        "            factor_data[f\"{ticker}_momentum\"] = momentum\n",
        "\n",
        "            # Volatility: 30-day rolling standard deviation\n",
        "            volatility = ticker_data.pct_change().rolling(window=30).std() * np.sqrt(252)\n",
        "            factor_data[f\"{ticker}_volatility\"] = volatility\n",
        "        except Exception as e:\n",
        "            print(f\"Error calculating factor-specific data for {ticker}: {e}\")\n",
        "\n",
        "    if not factor_data:\n",
        "        print(\"Warning: No factor-specific data collected.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    factor_df = pd.concat(factor_data, axis=1)\n",
        "    save_to_csv(factor_df, \"factor_data\")\n",
        "\n",
        "    return factor_df\n",
        "\n",
        "# 3. Macroeconomic Data (FRED API) - Fixed PMI issue\n",
        "def collect_macro_data():\n",
        "    macro_series = {\n",
        "        \"CPI_YoY\": \"CPIAUCSL\",\n",
        "        \"10Y_2Y_Spread\": \"T10Y2Y\",\n",
        "        \"Unemployment_Rate\": \"UNRATE\",\n",
        "        \"TED_Spread\": \"TEDRATE\",\n",
        "        \"Industrial_Production\": \"INDPRO\"\n",
        "    }\n",
        "\n",
        "    macro_data = {}\n",
        "    unavailable_series = []\n",
        "    for name, series_id in macro_series.items():\n",
        "        print(f\"Collecting macro data: {name}\")\n",
        "        try:\n",
        "            data = fred.get_series(series_id, START_DATE, END_DATE)\n",
        "            data.index = pd.to_datetime(data.index)\n",
        "            macro_data[name] = data\n",
        "        except Exception as e:\n",
        "            print(f\"Error collecting {name}: {e}\")\n",
        "            unavailable_series.append(name)\n",
        "\n",
        "    # Calculate YoY CPI change\n",
        "    if \"CPI_YoY\" in macro_data:\n",
        "        macro_data[\"CPI_YoY\"] = macro_data[\"CPI_YoY\"].pct_change(periods=12) * 100\n",
        "\n",
        "    if not macro_data:\n",
        "        print(\"Warning: No macro data collected.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    if unavailable_series:\n",
        "        print(f\"Unavailable macro series: {unavailable_series}\")\n",
        "\n",
        "    macro_df = pd.concat(macro_data, axis=1)\n",
        "    macro_df = macro_df.ffill()  # Forward-fill to align frequencies\n",
        "    save_to_csv(macro_df, \"macro_data\")\n",
        "\n",
        "    return macro_df\n",
        "\n",
        "# 4. Volatility Data\n",
        "def collect_volatility_data():\n",
        "    print(\"Collecting VIX data\")\n",
        "    try:\n",
        "        vix_data = yf.download(\"^VIX\", start=START_DATE, end=END_DATE, auto_adjust=False, progress=False)\n",
        "        if vix_data.empty or 'Adj Close' not in vix_data.columns:\n",
        "            raise ValueError(\"No VIX data or 'Adj Close' missing\")\n",
        "        vix_df = pd.DataFrame(vix_data[\"Adj Close\"], columns=[\"VIX\"])\n",
        "        save_to_csv(vix_df, \"volatility_data\")\n",
        "        return vix_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting VIX data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# 5. Alternative Data\n",
        "def collect_alternative_data(price_df):\n",
        "    # Google Trends data collection (disabled by default)\n",
        "    google_trends_df = pd.DataFrame()\n",
        "    if use_google_trends:\n",
        "        from pytrends.request import TrendReq\n",
        "        pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
        "        keywords = {\n",
        "            \"XLK\": \"tech stocks\",\n",
        "            \"XLE\": \"energy crisis\",\n",
        "            \"XLV\": \"healthcare stocks\",\n",
        "            \"XLU\": \"utilities stocks\"\n",
        "        }\n",
        "        google_trends_data = {}\n",
        "        for ticker, keyword in keywords.items():\n",
        "            print(f\"Collecting Google Trends data for {keyword}\")\n",
        "            retries = 3\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    pytrends.build_payload([keyword], timeframe=f\"{START_DATE} {END_DATE}\")\n",
        "                    data = pytrends.interest_over_time()\n",
        "                    if not data.empty:\n",
        "                        google_trends_data[ticker] = data[keyword]\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error collecting Google Trends for {keyword} (attempt {attempt + 1}/{retries}): {e}\")\n",
        "                    if \"code 429\" in str(e):\n",
        "                        time.sleep(60)\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "        if google_trends_data:\n",
        "            google_trends_df = pd.concat(google_trends_data, axis=1)\n",
        "            save_to_csv(google_trends_df, \"google_trends_data\")\n",
        "        else:\n",
        "            print(\"Warning: No Google Trends data collected.\")\n",
        "\n",
        "    # Credit Spreads (FRED)\n",
        "    print(\"Collecting credit spreads\")\n",
        "    try:\n",
        "        credit_spread = fred.get_series(\"BAMLH0A0HYM2\", START_DATE, END_DATE)\n",
        "        credit_spread_df = pd.DataFrame(credit_spread, columns=[\"Credit_Spread\"])\n",
        "        credit_spread_df.index = pd.to_datetime(credit_spread_df.index)\n",
        "        if not price_df.empty:\n",
        "            credit_spread_df = credit_spread_df.reindex(price_df.index, method='ffill')\n",
        "        save_to_csv(credit_spread_df, \"credit_spreads\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting credit spreads: {e}\")\n",
        "        credit_spread_df = pd.DataFrame()\n",
        "\n",
        "    # Sentiment Scores (Placeholder aligned with price data index)\n",
        "    sentiment_data = {}\n",
        "    if not price_df.empty:\n",
        "        date_range = price_df.index\n",
        "        for ticker in ETF_UNIVERSE:\n",
        "            sentiment_data[ticker] = pd.Series(np.random.uniform(0, 1, len(date_range)), index=date_range)\n",
        "        sentiment_df = pd.DataFrame(sentiment_data)\n",
        "        save_to_csv(sentiment_df, \"sentiment_data\")\n",
        "    else:\n",
        "        sentiment_df = pd.DataFrame()\n",
        "        print(\"Warning: No sentiment data generated due to empty price data.\")\n",
        "\n",
        "    return google_trends_df, credit_spread_df, sentiment_df\n",
        "\n",
        "# 6. Cross-Asset Data\n",
        "def collect_cross_asset_data():\n",
        "    price_data = {}\n",
        "    for ticker in CROSS_ASSETS:\n",
        "        print(f\"Collecting cross-asset data for {ticker}\")\n",
        "        try:\n",
        "            data = yf.download(ticker, start=START_DATE, end=END_DATE, auto_adjust=False, progress=False)\n",
        "            if data.empty or 'Adj Close' not in data.columns:\n",
        "                raise ValueError(f\"No data or 'Adj Close' missing for {ticker}\")\n",
        "            price_data[ticker] = data[\"Adj Close\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error collecting cross-asset data for {ticker}: {e}\")\n",
        "\n",
        "    if not price_data:\n",
        "        print(\"Warning: No cross-asset data collected.\")\n",
        "        return pd.DataFrame(index=pd.to_datetime([]))  # Create an empty DataFrame with DatetimeIndex\n",
        "\n",
        "    cross_asset_df = pd.concat(price_data, axis=1)\n",
        "    save_to_csv(cross_asset_df, \"cross_asset_data\")\n",
        "\n",
        "    return cross_asset_df\n",
        "\n",
        "# 7. ETF Metadata\n",
        "def collect_etf_metadata():\n",
        "    metadata = {}\n",
        "    for ticker in ETF_UNIVERSE:\n",
        "        print(f\"Collecting metadata for {ticker}\")\n",
        "        try:\n",
        "            ticker_obj = yf.Ticker(ticker)\n",
        "            info = ticker_obj.info\n",
        "            metadata[ticker] = {\n",
        "                \"Avg_Daily_Volume\": info.get(\"averageDailyVolume10Day\", np.nan),\n",
        "                \"Expense_Ratio\": info.get(\"annualHoldingsTurnover\", 0.15),\n",
        "                \"Bid_Ask_Spread\": (info.get(\"bid\", np.nan) - info.get(\"ask\", np.nan)) if info.get(\"bid\") and info.get(\"ask\") else np.nan\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Error collecting metadata for {ticker}: {e}\")\n",
        "\n",
        "    if not metadata:\n",
        "        print(\"Warning: No ETF metadata collected.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    metadata_df = pd.DataFrame(metadata).T\n",
        "    save_to_csv(metadata_df, \"etf_metadata\")\n",
        "\n",
        "    return metadata_df\n",
        "\n",
        "# Main function to collect all data\n",
        "def collect_all_data():\n",
        "    print(\"Starting data collection...\")\n",
        "\n",
        "    # Collect price/volume data once and cache\n",
        "    price_df, volume_df, avg_daily_volume, failed_tickers, missing_data_tickers = collect_price_volume_data(ETF_UNIVERSE + CROSS_ASSETS)\n",
        "    successful_tickers = [ticker for ticker in (ETF_UNIVERSE + CROSS_ASSETS) if ticker not in failed_tickers]\n",
        "\n",
        "    factor_df = collect_factor_specific_data(price_df, [ticker for ticker in ETF_UNIVERSE if ticker not in failed_tickers])\n",
        "    macro_df = collect_macro_data()\n",
        "    volatility_df = collect_volatility_data()\n",
        "    google_trends_df, credit_spread_df, sentiment_df = collect_alternative_data(price_df)\n",
        "    cross_asset_df = collect_cross_asset_data()\n",
        "    metadata_df = collect_etf_metadata()\n",
        "\n",
        "    print(\"Data collection completed.\")\n",
        "\n",
        "    return {\n",
        "        \"price_data\": price_df,\n",
        "        \"volume_data\": volume_df,\n",
        "        \"avg_daily_volume\": avg_daily_volume,\n",
        "        \"factor_data\": factor_df,\n",
        "        \"macro_data\": macro_df,\n",
        "        \"volatility_data\": volatility_df,\n",
        "        \"google_trends_data\": google_trends_df,\n",
        "        \"credit_spread_data\": credit_spread_df,\n",
        "        \"sentiment_data\": sentiment_df,\n",
        "        \"cross_asset_data\": cross_asset_df,\n",
        "        \"metadata_data\": metadata_df,\n",
        "        \"failed_tickers\": failed_tickers,\n",
        "        \"missing_data_tickers\": missing_data_tickers\n",
        "    }\n",
        "\n",
        "# Run the data collection\n",
        "if __name__ == \"__main__\":\n",
        "    data_dict = collect_all_data()\n",
        "\n",
        "    print(\"\\nSample Price Data:\")\n",
        "    print(data_dict[\"price_data\"].tail())\n",
        "    print(\"\\nSample Macro Data:\")\n",
        "    print(data_dict[\"macro_data\"].head())\n",
        "    print(\"\\nFailed Tickers (if any):\")\n",
        "    print(data_dict[\"failed_tickers\"])\n",
        "    print(\"\\nMissing Data Tickers (if any):\")\n",
        "    print(data_dict[\"missing_data_tickers\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmhNrK2iL9Ey",
        "outputId": "0c312045-15a9-417d-a2b0-b978c0dfd0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data collection...\n",
            "Collecting price/volume data for MTUM\n",
            "Collecting price/volume data for VLUE\n",
            "Collecting price/volume data for QUAL\n",
            "Collecting price/volume data for USMV\n",
            "Collecting price/volume data for XLK\n",
            "Collecting price/volume data for XLV\n",
            "Collecting price/volume data for XLE\n",
            "Collecting price/volume data for XLU\n",
            "Collecting price/volume data for EEM\n",
            "Collecting price/volume data for EFA\n",
            "Collecting price/volume data for TLT\n",
            "Collecting price/volume data for IEF\n",
            "Collecting price/volume data for TIP\n",
            "Collecting price/volume data for BIL\n",
            "Collecting price/volume data for SPY\n",
            "Collecting price/volume data for TLT\n",
            "Collecting price/volume data for GLD\n",
            "Saved price_data to etf_data/price_data.csv\n",
            "Saved volume_data to etf_data/volume_data.csv\n",
            "Saved avg_daily_volume to etf_data/avg_daily_volume.csv\n",
            "Calculating factor-specific data for MTUM\n",
            "Calculating factor-specific data for VLUE\n",
            "Calculating factor-specific data for QUAL\n",
            "Calculating factor-specific data for USMV\n",
            "Calculating factor-specific data for XLK\n",
            "Calculating factor-specific data for XLV\n",
            "Calculating factor-specific data for XLE\n",
            "Calculating factor-specific data for XLU\n",
            "Calculating factor-specific data for EEM\n",
            "Calculating factor-specific data for EFA\n",
            "Calculating factor-specific data for TLT\n",
            "Calculating factor-specific data for IEF\n",
            "Calculating factor-specific data for TIP\n",
            "Calculating factor-specific data for BIL\n",
            "Saved factor_data to etf_data/factor_data.csv\n",
            "Collecting macro data: CPI_YoY\n",
            "Collecting macro data: 10Y_2Y_Spread\n",
            "Collecting macro data: Unemployment_Rate\n",
            "Collecting macro data: TED_Spread\n",
            "Collecting macro data: Industrial_Production\n",
            "Saved macro_data to etf_data/macro_data.csv\n",
            "Collecting VIX data\n",
            "Saved volatility_data to etf_data/volatility_data.csv\n",
            "Collecting credit spreads\n",
            "Saved credit_spreads to etf_data/credit_spreads.csv\n",
            "Saved sentiment_data to etf_data/sentiment_data.csv\n",
            "Collecting cross-asset data for SPY\n",
            "Collecting cross-asset data for TLT\n",
            "Collecting cross-asset data for GLD\n",
            "Saved cross_asset_data to etf_data/cross_asset_data.csv\n",
            "Collecting metadata for MTUM\n",
            "Collecting metadata for VLUE\n",
            "Collecting metadata for QUAL\n",
            "Collecting metadata for USMV\n",
            "Collecting metadata for XLK\n",
            "Collecting metadata for XLV\n",
            "Collecting metadata for XLE\n",
            "Collecting metadata for XLU\n",
            "Collecting metadata for EEM\n",
            "Collecting metadata for EFA\n",
            "Collecting metadata for TLT\n",
            "Collecting metadata for IEF\n",
            "Collecting metadata for TIP\n",
            "Collecting metadata for BIL\n",
            "Saved etf_metadata to etf_data/etf_metadata.csv\n",
            "Data collection completed.\n",
            "\n",
            "Sample Price Data:\n",
            "                  MTUM                                                  \\\n",
            "Price             Open        High         Low       Close   Adj Close   \n",
            "Ticker            MTUM        MTUM        MTUM        MTUM        MTUM   \n",
            "Date                                                                     \n",
            "2025-02-27  219.639999  220.460007  214.250000  214.460007  214.460007   \n",
            "2025-02-28  214.259995  218.720001  213.850006  218.429993  218.429993   \n",
            "2025-03-03  220.050003  220.580002  212.539993  214.350006  214.350006   \n",
            "2025-03-04  211.509995  214.139999  206.440002  209.940002  209.940002   \n",
            "2025-03-05  210.490005  213.259995  208.679993  212.619995  212.619995   \n",
            "\n",
            "                  VLUE                                                  ...  \\\n",
            "Price             Open        High         Low       Close   Adj Close  ...   \n",
            "Ticker            VLUE        VLUE        VLUE        VLUE        VLUE  ...   \n",
            "Date                                                                    ...   \n",
            "2025-02-27  111.190002  111.750000  109.959999  110.000000  110.000000  ...   \n",
            "2025-02-28  109.870003  111.230003  109.339996  111.160004  111.160004  ...   \n",
            "2025-03-03  111.660004  112.129997  108.889999  109.580002  109.580002  ...   \n",
            "2025-03-04  108.830002  108.860001  106.849998  107.190002  107.190002  ...   \n",
            "2025-03-05  107.230003  108.550003  106.550003  108.160004  108.160004  ...   \n",
            "\n",
            "                   SPY                                                  \\\n",
            "Price             Open        High         Low       Close   Adj Close   \n",
            "Ticker             SPY         SPY         SPY         SPY         SPY   \n",
            "Date                                                                     \n",
            "2025-02-27  596.849976  598.020020  584.650024  585.049988  585.049988   \n",
            "2025-02-28  585.559998  594.719971  582.440002  594.179993  594.179993   \n",
            "2025-03-03  596.179993  597.340027  579.900024  583.770020  583.770020   \n",
            "2025-03-04  579.710022  585.390015  572.250000  576.859985  576.859985   \n",
            "2025-03-05  576.690002  584.880005  573.080017  583.059998  583.059998   \n",
            "\n",
            "                   GLD                                                  \n",
            "Price             Open        High         Low       Close   Adj Close  \n",
            "Ticker             GLD         GLD         GLD         GLD         GLD  \n",
            "Date                                                                    \n",
            "2025-02-27  266.489990  266.670013  264.450012  264.929993  264.929993  \n",
            "2025-02-28  262.559998  263.399994  261.250000  263.269989  263.269989  \n",
            "2025-03-03  265.070007  266.880005  265.070007  266.739990  266.739990  \n",
            "2025-03-04  269.010010  269.399994  267.519989  269.059998  269.059998  \n",
            "2025-03-05  267.970001  270.260010  267.959991  269.619995  269.619995  \n",
            "\n",
            "[5 rows x 80 columns]\n",
            "\n",
            "Sample Macro Data:\n",
            "            CPI_YoY  10Y_2Y_Spread  Unemployment_Rate  TED_Spread  \\\n",
            "2010-01-01      NaN            NaN                9.8         NaN   \n",
            "2010-01-04      NaN           2.76                9.8        0.17   \n",
            "2010-01-05      NaN           2.76                9.8        0.18   \n",
            "2010-01-06      NaN           2.84                9.8        0.19   \n",
            "2010-01-07      NaN           2.82                9.8        0.20   \n",
            "\n",
            "            Industrial_Production  \n",
            "2010-01-01                89.1897  \n",
            "2010-01-04                89.1897  \n",
            "2010-01-05                89.1897  \n",
            "2010-01-06                89.1897  \n",
            "2010-01-07                89.1897  \n",
            "\n",
            "Failed Tickers (if any):\n",
            "[]\n",
            "\n",
            "Missing Data Tickers (if any):\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering -- Not done"
      ],
      "metadata": {
        "id": "riWsKr2J5FQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import os\n",
        "\n",
        "# Check if running in Google Colab\n",
        "try:\n",
        "    from google.colab import files\n",
        "    IN_COLAB = True\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "def preprocess_and_engineer_features(data_dict):\n",
        "    \"\"\"\n",
        "    Preprocess data and engineer features with explicit NaN handling and optimization\n",
        "    \"\"\"\n",
        "    price_df = data_dict['price_data']\n",
        "    macro_df = data_dict['macro_data']\n",
        "    volatility_df = data_dict['volatility_data']\n",
        "    volume_df = data_dict['volume_data']\n",
        "\n",
        "    print(\"Price_df index:\", price_df.index[:5])\n",
        "    print(\"Macro_df index:\", macro_df.index[:5])\n",
        "    print(\"Volatility_df index:\", volatility_df.index[:5])\n",
        "    print(\"Price_df columns:\", price_df.columns[:5])\n",
        "    print(\"Macro_df columns:\", macro_df.columns)\n",
        "\n",
        "    features_df = pd.DataFrame(index=price_df.index)\n",
        "\n",
        "    # 1. Momentum Features\n",
        "    def calculate_momentum_features(ticker):\n",
        "        data = price_df[ticker]['Adj Close'].squeeze().reindex(features_df.index, method='ffill')\n",
        "        print(f\"{ticker} Adj Close shape:\", data.shape)\n",
        "\n",
        "        features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
        "        features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
        "        features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
        "        features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
        "\n",
        "        sma200 = data.rolling(200).mean().reindex(features_df.index, method='ffill')\n",
        "        features_df[f'{ticker}_SMA200'] = sma200\n",
        "\n",
        "        data_aligned, sma200_aligned = data.align(sma200, join='inner', axis=0)\n",
        "        features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
        "\n",
        "        mom_12, mom_3 = features_df[f'{ticker}_12m_mom'].align(features_df[f'{ticker}_3m_mom'], join='inner')\n",
        "        features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    # 2. Risk Features\n",
        "    def calculate_risk_features(ticker):\n",
        "        data = price_df[ticker]\n",
        "        print(f\"{ticker} data columns:\", data.columns)\n",
        "        returns = data['Adj Close'].squeeze().pct_change().reindex(features_df.index, method='ffill')\n",
        "        features_df[f'{ticker}_Returns'] = returns\n",
        "        features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
        "\n",
        "        spy_returns = price_df['SPY']['Adj Close'].squeeze().pct_change().reindex(features_df.index, method='ffill')\n",
        "        returns, spy_returns = returns.align(spy_returns, join='inner', axis=0)\n",
        "        print(f\"{ticker} returns shape after align:\", returns.shape)\n",
        "\n",
        "        rolling_returns = pd.concat([returns, spy_returns], axis=1).dropna()\n",
        "        if len(rolling_returns) >= 252:\n",
        "            model = sm.OLS(rolling_returns.iloc[:, 0],\n",
        "                          sm.add_constant(rolling_returns.iloc[:, 1])).fit()\n",
        "            features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
        "\n",
        "        covariance = returns.rolling(63).cov(spy_returns)\n",
        "        spy_variance = spy_returns.rolling(63).var()\n",
        "        print(f\"{ticker} covariance shape:\", covariance.shape)\n",
        "        print(f\"{ticker} spy_variance shape:\", spy_variance.shape)\n",
        "        beta63 = covariance.div(spy_variance).reindex(features_df.index, method='ffill')\n",
        "        print(f\"{ticker} Beta63 shape:\", beta63.shape)\n",
        "        features_df[f'{ticker}_Beta63'] = beta63\n",
        "\n",
        "        features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
        "            lambda x: np.percentile(x[x < np.percentile(x, 5)], 5) if len(x.dropna()) > 0 else np.nan\n",
        "        )\n",
        "\n",
        "        adj_close = data['Adj Close'].squeeze().reindex(features_df.index, method='ffill')\n",
        "        features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
        "        features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
        "        features_df[f'{ticker}_Time_Underwater'] = (\n",
        "            (adj_close < features_df[f'{ticker}_Peak']).astype(int).cumsum()\n",
        "        )\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    # 3. Macro Regime Features\n",
        "    def calculate_macro_features():\n",
        "        macro_aligned = macro_df.reindex(features_df.index, method='ffill')\n",
        "\n",
        "        macro_aligned['YC_Slope'] = macro_aligned['10Y_2Y_Spread'].rolling(21).mean()\n",
        "        macro_aligned['YC_Signal'] = (macro_aligned['YC_Slope'] > 0).astype(int)\n",
        "\n",
        "        try:\n",
        "            t10y3m = fred.get_series('T10Y3M', START_DATE, END_DATE)\n",
        "            macro_aligned['T10Y3M'] = t10y3m.reindex(macro_aligned.index, method='ffill')\n",
        "            macro_aligned['T10Y3M_Signal'] = (macro_aligned['T10Y3M'] > 0).astype(int)\n",
        "        except:\n",
        "            print(\"T10Y3M data not available\")\n",
        "\n",
        "        vix_aligned = volatility_df['VIX'].reindex(features_df.index, method='ffill')\n",
        "        macro_aligned['VIX_Regime'] = pd.cut(\n",
        "            vix_aligned,\n",
        "            bins=[0, 12, 20, 30, 100],\n",
        "            labels=[0, 1, 2, 3],\n",
        "            include_lowest=True\n",
        "        )\n",
        "\n",
        "        macro_aligned['CPI_Mom'] = macro_aligned['CPI_YoY'].diff(21) / 21\n",
        "        macro_aligned['PCE_Signal'] = (macro_aligned['CPI_YoY'] > 0.02).astype(int)\n",
        "\n",
        "        return macro_aligned\n",
        "\n",
        "    # 4. Relative Strength & Correlation Features\n",
        "    def calculate_relative_features(ticker):\n",
        "        spy_12m_mom = features_df['SPY_12m_mom']\n",
        "        ticker_12m_mom = features_df[f'{ticker}_12m_mom']\n",
        "        ticker_12m_mom, spy_12m_mom = ticker_12m_mom.align(spy_12m_mom, join='inner', axis=0)\n",
        "        features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
        "\n",
        "        spy_returns = price_df['SPY']['Adj Close'].squeeze().pct_change().reindex(features_df.index, method='ffill')\n",
        "        ticker_returns = price_df[ticker]['Adj Close'].squeeze().pct_change().reindex(features_df.index, method='ffill')\n",
        "        ticker_returns, spy_returns = ticker_returns.align(spy_returns, join='inner', axis=0)\n",
        "\n",
        "        features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
        "\n",
        "        if ticker == 'MTUM':\n",
        "            vlue_returns = price_df['VLUE']['Adj Close'].squeeze().pct_change().reindex(features_df.index, method='ffill')\n",
        "            ticker_returns, vlue_returns = ticker_returns.align(vlue_returns, join='inner', axis=0)\n",
        "            features_df['Corr_MTUM_VLUE'] = ticker_returns.rolling(63).corr(vlue_returns)\n",
        "\n",
        "        return features_df\n",
        "\n",
        "    # Process all tickers\n",
        "    tickers = [t for t in ETF_UNIVERSE + CROSS_ASSETS if t in price_df.columns.get_level_values(0)]\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"Processing {ticker} - Momentum and Risk\")\n",
        "        features_df = calculate_momentum_features(ticker)\n",
        "        features_df = calculate_risk_features(ticker)\n",
        "\n",
        "    for ticker in tickers:\n",
        "        print(f\"Processing {ticker} - Relative Features\")\n",
        "        features_df = calculate_relative_features(ticker)\n",
        "\n",
        "    macro_df = calculate_macro_features()\n",
        "    features_df = features_df.join(macro_df, how='left')\n",
        "\n",
        "    # Normalization and Dimensionality Reduction\n",
        "    ticker_specific_cols = [\n",
        "        '12m_mom', '6m_mom', '3m_mom', 'Mom_Accel', 'Vol30',\n",
        "        'Beta63', 'CVaR', 'DD', 'RS_SPY', 'Corr_SPY'\n",
        "    ]\n",
        "    macro_cols = ['YC_Slope', 'VIX_Regime', 'CPI_Mom']\n",
        "\n",
        "    feature_dict = {}\n",
        "    for ticker in tickers:\n",
        "        for col in ticker_specific_cols:\n",
        "            feature_dict[f'{ticker}_{col}'] = features_df[f'{ticker}_{col}']\n",
        "    for col in macro_cols:\n",
        "        feature_dict[col] = features_df[col]\n",
        "\n",
        "    feature_matrix = pd.DataFrame(feature_dict, index=features_df.index)\n",
        "\n",
        "    print(\"Feature matrix NaN count before handling:\", feature_matrix.isna().sum().sum())\n",
        "    feature_matrix = feature_matrix.ffill().bfill()\n",
        "    print(\"Feature matrix NaN count after ffill/bfill:\", feature_matrix.isna().sum().sum())\n",
        "    print(\"Columns with NaNs after ffill/bfill:\", feature_matrix.columns[feature_matrix.isna().any()].tolist())\n",
        "    feature_matrix = feature_matrix.fillna(0)\n",
        "    print(\"Feature matrix shape after imputation:\", feature_matrix.shape)\n",
        "\n",
        "    if feature_matrix.empty:\n",
        "        raise ValueError(\"Feature matrix is empty after processing. Check data availability.\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    normalized_features = scaler.fit_transform(feature_matrix)\n",
        "    normalized_df = pd.DataFrame(normalized_features, index=feature_matrix.index, columns=feature_matrix.columns)\n",
        "\n",
        "    robust_scaler = RobustScaler()\n",
        "    robust_features = robust_scaler.fit_transform(feature_matrix)\n",
        "    robust_df = pd.DataFrame(robust_features, index=feature_matrix.index, columns=feature_matrix.columns)\n",
        "\n",
        "    pca = PCA(n_components=15)\n",
        "    pca_features = pca.fit_transform(normalized_features)\n",
        "    pca_df = pd.DataFrame(pca_features, index=feature_matrix.index, columns=[f'PC_{i+1}' for i in range(15)])\n",
        "\n",
        "    # Save datasets to CSV\n",
        "    output_dir = \"etf_features\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    raw_file = os.path.join(output_dir, \"raw_features.csv\")\n",
        "    normalized_file = os.path.join(output_dir, \"normalized_features.csv\")\n",
        "    robust_file = os.path.join(output_dir, \"robust_features.csv\")\n",
        "    pca_file = os.path.join(output_dir, \"pca_features.csv\")\n",
        "\n",
        "    features_df.to_csv(raw_file)\n",
        "    normalized_df.to_csv(normalized_file)\n",
        "    robust_df.to_csv(robust_file)\n",
        "    pca_df.to_csv(pca_file)\n",
        "\n",
        "    print(f\"Saved raw features to: {raw_file}\")\n",
        "    print(f\"Saved normalized features to: {normalized_file}\")\n",
        "    print(f\"Saved robust features to: {robust_file}\")\n",
        "    print(f\"Saved PCA features to: {pca_file}\")\n",
        "\n",
        "    # Combine all into a single dataset\n",
        "    combined_df = pd.concat([features_df, normalized_df.add_suffix('_norm'),\n",
        "                             robust_df.add_suffix('_robust'), pca_df], axis=1)\n",
        "    combined_file = os.path.join(output_dir, \"combined_features.csv\")\n",
        "    combined_df.to_csv(combined_file)\n",
        "    print(f\"Saved combined features to: {combined_file}\")\n",
        "\n",
        "    # Download if in Google Colab\n",
        "    if IN_COLAB:\n",
        "        #files.download(combined_file)\n",
        "        print(f\"Downloading {combined_file} in Google Colab\")\n",
        "    else:\n",
        "        print(f\"File saved locally at {combined_file}. Use your local file explorer to access it.\")\n",
        "\n",
        "    return {\n",
        "        'raw_features': features_df,\n",
        "        'normalized_features': normalized_df,\n",
        "        'robust_features': robust_df,\n",
        "        'pca_features': pca_df,\n",
        "        'pca_explained_variance': pca.explained_variance_ratio_\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    feature_dict = preprocess_and_engineer_features(data_dict)\n",
        "\n",
        "    print(\"\\nSample Raw Features:\")\n",
        "    print(feature_dict['raw_features'].tail())\n",
        "    print(\"\\nSample Normalized Features:\")\n",
        "    print(feature_dict['normalized_features'].tail())\n",
        "    print(\"\\nExplained Variance Ratio from PCA:\")\n",
        "    print(feature_dict['pca_explained_variance'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JPM8rF-NP04h",
        "outputId": "7cbd16ff-756c-4c77-d33a-12aeacfdc10f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price_df index: DatetimeIndex(['2010-01-04', '2010-01-05', '2010-01-06', '2010-01-07',\n",
            "               '2010-01-08'],\n",
            "              dtype='datetime64[ns]', name='Date', freq=None)\n",
            "Macro_df index: DatetimeIndex(['2010-01-01', '2010-01-04', '2010-01-05', '2010-01-06',\n",
            "               '2010-01-07'],\n",
            "              dtype='datetime64[ns]', freq=None)\n",
            "Volatility_df index: DatetimeIndex(['2010-01-04', '2010-01-05', '2010-01-06', '2010-01-07',\n",
            "               '2010-01-08'],\n",
            "              dtype='datetime64[ns]', name='Date', freq=None)\n",
            "Price_df columns: MultiIndex([('MTUM',      'Open', 'MTUM'),\n",
            "            ('MTUM',      'High', 'MTUM'),\n",
            "            ('MTUM',       'Low', 'MTUM'),\n",
            "            ('MTUM',     'Close', 'MTUM'),\n",
            "            ('MTUM', 'Adj Close', 'MTUM')],\n",
            "           names=[None, 'Price', 'Ticker'])\n",
            "Macro_df columns: Index(['CPI_YoY', '10Y_2Y_Spread', 'Unemployment_Rate', 'TED_Spread',\n",
            "       'Industrial_Production'],\n",
            "      dtype='object')\n",
            "Processing MTUM - Momentum and Risk\n",
            "MTUM Adj Close shape: (3816,)\n",
            "MTUM data columns: MultiIndex([(     'Open', 'MTUM'),\n",
            "            (     'High', 'MTUM'),\n",
            "            (      'Low', 'MTUM'),\n",
            "            (    'Close', 'MTUM'),\n",
            "            ('Adj Close', 'MTUM')],\n",
            "           names=['Price', 'Ticker'])\n",
            "MTUM returns shape after align: (3816,)\n",
            "MTUM covariance shape: (3816,)\n",
            "MTUM spy_variance shape: (3816,)\n",
            "MTUM Beta63 shape: (3816,)\n",
            "Processing VLUE - Momentum and Risk\n",
            "VLUE Adj Close shape: (3816,)\n",
            "VLUE data columns: MultiIndex([(     'Open', 'VLUE'),\n",
            "            (     'High', 'VLUE'),\n",
            "            (      'Low', 'VLUE'),\n",
            "            (    'Close', 'VLUE'),\n",
            "            ('Adj Close', 'VLUE')],\n",
            "           names=['Price', 'Ticker'])\n",
            "VLUE returns shape after align: (3816,)\n",
            "VLUE covariance shape: (3816,)\n",
            "VLUE spy_variance shape: (3816,)\n",
            "VLUE Beta63 shape: (3816,)\n",
            "Processing QUAL - Momentum and Risk\n",
            "QUAL Adj Close shape: (3816,)\n",
            "QUAL data columns: MultiIndex([(     'Open', 'QUAL'),\n",
            "            (     'High', 'QUAL'),\n",
            "            (      'Low', 'QUAL'),\n",
            "            (    'Close', 'QUAL'),\n",
            "            ('Adj Close', 'QUAL')],\n",
            "           names=['Price', 'Ticker'])\n",
            "QUAL returns shape after align: (3816,)\n",
            "QUAL covariance shape: (3816,)\n",
            "QUAL spy_variance shape: (3816,)\n",
            "QUAL Beta63 shape: (3816,)\n",
            "Processing USMV - Momentum and Risk\n",
            "USMV Adj Close shape: (3816,)\n",
            "USMV data columns: MultiIndex([(     'Open', 'USMV'),\n",
            "            (     'High', 'USMV'),\n",
            "            (      'Low', 'USMV'),\n",
            "            (    'Close', 'USMV'),\n",
            "            ('Adj Close', 'USMV')],\n",
            "           names=['Price', 'Ticker'])\n",
            "USMV returns shape after align: (3816,)\n",
            "USMV covariance shape: (3816,)\n",
            "USMV spy_variance shape: (3816,)\n",
            "USMV Beta63 shape: (3816,)\n",
            "Processing XLK - Momentum and Risk\n",
            "XLK Adj Close shape: (3816,)\n",
            "XLK data columns: MultiIndex([(     'Open', 'XLK'),\n",
            "            (     'High', 'XLK'),\n",
            "            (      'Low', 'XLK'),\n",
            "            (    'Close', 'XLK'),\n",
            "            ('Adj Close', 'XLK')],\n",
            "           names=['Price', 'Ticker'])\n",
            "XLK returns shape after align: (3816,)\n",
            "XLK covariance shape: (3816,)\n",
            "XLK spy_variance shape: (3816,)\n",
            "XLK Beta63 shape: (3816,)\n",
            "Processing XLV - Momentum and Risk\n",
            "XLV Adj Close shape: (3816,)\n",
            "XLV data columns: MultiIndex([(     'Open', 'XLV'),\n",
            "            (     'High', 'XLV'),\n",
            "            (      'Low', 'XLV'),\n",
            "            (    'Close', 'XLV'),\n",
            "            ('Adj Close', 'XLV')],\n",
            "           names=['Price', 'Ticker'])\n",
            "XLV returns shape after align: (3816,)\n",
            "XLV covariance shape: (3816,)\n",
            "XLV spy_variance shape: (3816,)\n",
            "XLV Beta63 shape: (3816,)\n",
            "Processing XLE - Momentum and Risk\n",
            "XLE Adj Close shape: (3816,)\n",
            "XLE data columns: MultiIndex([(     'Open', 'XLE'),\n",
            "            (     'High', 'XLE'),\n",
            "            (      'Low', 'XLE'),\n",
            "            (    'Close', 'XLE'),\n",
            "            ('Adj Close', 'XLE')],\n",
            "           names=['Price', 'Ticker'])\n",
            "XLE returns shape after align: (3816,)\n",
            "XLE covariance shape: (3816,)\n",
            "XLE spy_variance shape: (3816,)\n",
            "XLE Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n",
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing XLU - Momentum and Risk\n",
            "XLU Adj Close shape: (3816,)\n",
            "XLU data columns: MultiIndex([(     'Open', 'XLU'),\n",
            "            (     'High', 'XLU'),\n",
            "            (      'Low', 'XLU'),\n",
            "            (    'Close', 'XLU'),\n",
            "            ('Adj Close', 'XLU')],\n",
            "           names=['Price', 'Ticker'])\n",
            "XLU returns shape after align: (3816,)\n",
            "XLU covariance shape: (3816,)\n",
            "XLU spy_variance shape: (3816,)\n",
            "XLU Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing EEM - Momentum and Risk\n",
            "EEM Adj Close shape: (3816,)\n",
            "EEM data columns: MultiIndex([(     'Open', 'EEM'),\n",
            "            (     'High', 'EEM'),\n",
            "            (      'Low', 'EEM'),\n",
            "            (    'Close', 'EEM'),\n",
            "            ('Adj Close', 'EEM')],\n",
            "           names=['Price', 'Ticker'])\n",
            "EEM returns shape after align: (3816,)\n",
            "EEM covariance shape: (3816,)\n",
            "EEM spy_variance shape: (3816,)\n",
            "EEM Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing EFA - Momentum and Risk\n",
            "EFA Adj Close shape: (3816,)\n",
            "EFA data columns: MultiIndex([(     'Open', 'EFA'),\n",
            "            (     'High', 'EFA'),\n",
            "            (      'Low', 'EFA'),\n",
            "            (    'Close', 'EFA'),\n",
            "            ('Adj Close', 'EFA')],\n",
            "           names=['Price', 'Ticker'])\n",
            "EFA returns shape after align: (3816,)\n",
            "EFA covariance shape: (3816,)\n",
            "EFA spy_variance shape: (3816,)\n",
            "EFA Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TLT - Momentum and Risk\n",
            "TLT Adj Close shape: (3816,)\n",
            "TLT data columns: MultiIndex([(     'Open', 'TLT'),\n",
            "            (     'High', 'TLT'),\n",
            "            (      'Low', 'TLT'),\n",
            "            (    'Close', 'TLT'),\n",
            "            ('Adj Close', 'TLT')],\n",
            "           names=['Price', 'Ticker'])\n",
            "TLT returns shape after align: (3816,)\n",
            "TLT covariance shape: (3816,)\n",
            "TLT spy_variance shape: (3816,)\n",
            "TLT Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing IEF - Momentum and Risk\n",
            "IEF Adj Close shape: (3816,)\n",
            "IEF data columns: MultiIndex([(     'Open', 'IEF'),\n",
            "            (     'High', 'IEF'),\n",
            "            (      'Low', 'IEF'),\n",
            "            (    'Close', 'IEF'),\n",
            "            ('Adj Close', 'IEF')],\n",
            "           names=['Price', 'Ticker'])\n",
            "IEF returns shape after align: (3816,)\n",
            "IEF covariance shape: (3816,)\n",
            "IEF spy_variance shape: (3816,)\n",
            "IEF Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TIP - Momentum and Risk\n",
            "TIP Adj Close shape: (3816,)\n",
            "TIP data columns: MultiIndex([(     'Open', 'TIP'),\n",
            "            (     'High', 'TIP'),\n",
            "            (      'Low', 'TIP'),\n",
            "            (    'Close', 'TIP'),\n",
            "            ('Adj Close', 'TIP')],\n",
            "           names=['Price', 'Ticker'])\n",
            "TIP returns shape after align: (3816,)\n",
            "TIP covariance shape: (3816,)\n",
            "TIP spy_variance shape: (3816,)\n",
            "TIP Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing BIL - Momentum and Risk\n",
            "BIL Adj Close shape: (3816,)\n",
            "BIL data columns: MultiIndex([(     'Open', 'BIL'),\n",
            "            (     'High', 'BIL'),\n",
            "            (      'Low', 'BIL'),\n",
            "            (    'Close', 'BIL'),\n",
            "            ('Adj Close', 'BIL')],\n",
            "           names=['Price', 'Ticker'])\n",
            "BIL returns shape after align: (3816,)\n",
            "BIL covariance shape: (3816,)\n",
            "BIL spy_variance shape: (3816,)\n",
            "BIL Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing SPY - Momentum and Risk\n",
            "SPY Adj Close shape: (3816,)\n",
            "SPY data columns: MultiIndex([(     'Open', 'SPY'),\n",
            "            (     'High', 'SPY'),\n",
            "            (      'Low', 'SPY'),\n",
            "            (    'Close', 'SPY'),\n",
            "            ('Adj Close', 'SPY')],\n",
            "           names=['Price', 'Ticker'])\n",
            "SPY returns shape after align: (3816,)\n",
            "SPY covariance shape: (3816,)\n",
            "SPY spy_variance shape: (3816,)\n",
            "SPY Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing TLT - Momentum and Risk\n",
            "TLT Adj Close shape: (3816,)\n",
            "TLT data columns: MultiIndex([(     'Open', 'TLT'),\n",
            "            (     'High', 'TLT'),\n",
            "            (      'Low', 'TLT'),\n",
            "            (    'Close', 'TLT'),\n",
            "            ('Adj Close', 'TLT')],\n",
            "           names=['Price', 'Ticker'])\n",
            "TLT returns shape after align: (3816,)\n",
            "TLT covariance shape: (3816,)\n",
            "TLT spy_variance shape: (3816,)\n",
            "TLT Beta63 shape: (3816,)\n",
            "Processing GLD - Momentum and Risk\n",
            "GLD Adj Close shape: (3816,)\n",
            "GLD data columns: MultiIndex([(     'Open', 'GLD'),\n",
            "            (     'High', 'GLD'),\n",
            "            (      'Low', 'GLD'),\n",
            "            (    'Close', 'GLD'),\n",
            "            ('Adj Close', 'GLD')],\n",
            "           names=['Price', 'Ticker'])\n",
            "GLD returns shape after align: (3816,)\n",
            "GLD covariance shape: (3816,)\n",
            "GLD spy_variance shape: (3816,)\n",
            "GLD Beta63 shape: (3816,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-35-45b8aea9fafb>:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_12m_mom'] = data.pct_change(252)\n",
            "<ipython-input-35-45b8aea9fafb>:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_6m_mom'] = data.pct_change(126)\n",
            "<ipython-input-35-45b8aea9fafb>:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_3m_mom'] = data.pct_change(63)\n",
            "<ipython-input-35-45b8aea9fafb>:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Accel'] = features_df[f'{ticker}_12m_mom'].diff(21) / 21\n",
            "<ipython-input-35-45b8aea9fafb>:43: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_SMA200'] = sma200\n",
            "<ipython-input-35-45b8aea9fafb>:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Above_SMA'] = (data_aligned > sma200_aligned).astype(int).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:49: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Mom_Div'] = (mom_12 - mom_3).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Returns'] = returns\n",
            "<ipython-input-35-45b8aea9fafb>:59: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Vol30'] = returns.rolling(30).std() * np.sqrt(252)\n",
            "<ipython-input-35-45b8aea9fafb>:69: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta252'] = model.params.iloc[1]\n",
            "<ipython-input-35-45b8aea9fafb>:77: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Beta63'] = beta63\n",
            "<ipython-input-35-45b8aea9fafb>:79: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_CVaR'] = returns.rolling(252).apply(\n",
            "<ipython-input-35-45b8aea9fafb>:84: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Peak'] = adj_close.cummax()\n",
            "<ipython-input-35-45b8aea9fafb>:85: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_DD'] = (adj_close - features_df[f'{ticker}_Peak']) / features_df[f'{ticker}_Peak']\n",
            "<ipython-input-35-45b8aea9fafb>:86: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Time_Underwater'] = (\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:135: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df['Corr_MTUM_VLUE'] = ticker_returns.rolling(63).corr(vlue_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n",
            "<ipython-input-35-45b8aea9fafb>:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_RS_SPY'] = (ticker_12m_mom - spy_12m_mom).reindex(features_df.index, method='ffill')\n",
            "<ipython-input-35-45b8aea9fafb>:130: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  features_df[f'{ticker}_Corr_SPY'] = ticker_returns.rolling(63).corr(spy_returns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing MTUM - Relative Features\n",
            "Processing VLUE - Relative Features\n",
            "Processing QUAL - Relative Features\n",
            "Processing USMV - Relative Features\n",
            "Processing XLK - Relative Features\n",
            "Processing XLV - Relative Features\n",
            "Processing XLE - Relative Features\n",
            "Processing XLU - Relative Features\n",
            "Processing EEM - Relative Features\n",
            "Processing EFA - Relative Features\n",
            "Processing TLT - Relative Features\n",
            "Processing IEF - Relative Features\n",
            "Processing TIP - Relative Features\n",
            "Processing BIL - Relative Features\n",
            "Processing SPY - Relative Features\n",
            "Processing TLT - Relative Features\n",
            "Processing GLD - Relative Features\n",
            "Feature matrix NaN count before handling: 56073\n",
            "Feature matrix NaN count after ffill/bfill: 3816\n",
            "Columns with NaNs after ffill/bfill: ['VIX_Regime']\n",
            "Feature matrix shape after imputation: (3816, 163)\n",
            "Saved raw features to: etf_features/raw_features.csv\n",
            "Saved normalized features to: etf_features/normalized_features.csv\n",
            "Saved robust features to: etf_features/robust_features.csv\n",
            "Saved PCA features to: etf_features/pca_features.csv\n",
            "Saved combined features to: etf_features/combined_features.csv\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3c67ee2a-9f16-468b-a550-076f95c2085c\", \"combined_features.csv\", 43036038)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading etf_features/combined_features.csv in Google Colab\n",
            "\n",
            "Sample Raw Features:\n",
            "            MTUM_12m_mom  MTUM_6m_mom  MTUM_3m_mom  MTUM_Mom_Accel  \\\n",
            "Date                                                                 \n",
            "2025-02-27      0.196872     0.099977     0.001373       -0.004806   \n",
            "2025-02-28      0.218286     0.116638     0.022050       -0.004086   \n",
            "2025-03-03      0.198050     0.106013    -0.005388       -0.005355   \n",
            "2025-03-04      0.160908     0.085318    -0.021281       -0.007044   \n",
            "2025-03-05      0.148806     0.083357    -0.014601       -0.008747   \n",
            "\n",
            "            MTUM_SMA200  MTUM_Above_SMA  MTUM_Mom_Div  MTUM_Returns  \\\n",
            "Date                                                                  \n",
            "2025-02-27   202.255784               1      0.195499     -0.017140   \n",
            "2025-02-28   202.430974               1      0.196236      0.018512   \n",
            "2025-03-03   202.590432               1      0.203438     -0.018679   \n",
            "2025-03-04   202.721930               1      0.182188     -0.020574   \n",
            "2025-03-05   202.845421               1      0.163407      0.012766   \n",
            "\n",
            "            MTUM_Vol30  MTUM_Beta252  ...  Unemployment_Rate  TED_Spread  \\\n",
            "Date                                  ...                                  \n",
            "2025-02-27    0.207758      1.027097  ...                4.0        0.09   \n",
            "2025-02-28    0.207324      1.027097  ...                4.0        0.09   \n",
            "2025-03-03    0.214128      1.027097  ...                4.0        0.09   \n",
            "2025-03-04    0.218153      1.027097  ...                4.0        0.09   \n",
            "2025-03-05    0.216328      1.027097  ...                4.0        0.09   \n",
            "\n",
            "            Industrial_Production  YC_Slope  YC_Signal  T10Y3M  T10Y3M_Signal  \\\n",
            "Date                                                                            \n",
            "2025-02-27                103.511  0.255714          1   -0.03              0   \n",
            "2025-02-28                103.511  0.251429          1   -0.08              0   \n",
            "2025-03-03                103.511  0.244762          1   -0.19              0   \n",
            "2025-03-04                103.511  0.240000          1   -0.12              0   \n",
            "2025-03-05                103.511  0.240476          1   -0.07              0   \n",
            "\n",
            "            VIX_Regime  CPI_Mom  PCE_Signal  \n",
            "Date                                         \n",
            "2025-02-27         NaN      0.0           1  \n",
            "2025-02-28         NaN      0.0           1  \n",
            "2025-03-03         NaN      0.0           1  \n",
            "2025-03-04         NaN      0.0           1  \n",
            "2025-03-05         NaN      0.0           1  \n",
            "\n",
            "[5 rows x 285 columns]\n",
            "\n",
            "Sample Normalized Features:\n",
            "            MTUM_12m_mom  MTUM_6m_mom  MTUM_3m_mom  MTUM_Mom_Accel  \\\n",
            "Date                                                                 \n",
            "2025-02-27      0.268183     0.215268    -0.640543       -1.357916   \n",
            "2025-02-28      0.422007     0.395084    -0.329090       -1.129739   \n",
            "2025-03-03      0.276643     0.280412    -0.742390       -1.531703   \n",
            "2025-03-04      0.009839     0.057055    -0.981768       -2.066598   \n",
            "2025-03-05     -0.077095     0.035885    -0.881151       -2.605718   \n",
            "\n",
            "            MTUM_Vol30  MTUM_Beta63  MTUM_CVaR   MTUM_DD  MTUM_RS_SPY  \\\n",
            "Date                                                                    \n",
            "2025-02-27    0.587335     1.166199  -0.050359 -0.180303     0.461387   \n",
            "2025-02-28    0.582344     1.159902  -0.050359  0.035287     0.533813   \n",
            "2025-03-03    0.660577     1.097606  -0.050359 -0.186277     0.520519   \n",
            "2025-03-04    0.706862     1.158158  -0.050359 -0.425762     0.260527   \n",
            "2025-03-05    0.685878     1.158718  -0.050359 -0.280225     0.075517   \n",
            "\n",
            "            MTUM_Corr_SPY  ...  GLD_Mom_Accel  GLD_Vol30  GLD_Beta63  \\\n",
            "Date                       ...                                         \n",
            "2025-02-27      -0.045429  ...       0.739018  -0.164288    1.018311   \n",
            "2025-02-28       0.059153  ...       0.691239  -0.138559    0.950662   \n",
            "2025-03-03       0.106306  ...       0.769522  -0.078001    0.755157   \n",
            "2025-03-04       0.149580  ...       0.835269  -0.089025    0.682452   \n",
            "2025-03-05       0.174838  ...       0.335183  -0.193780    0.643368   \n",
            "\n",
            "            GLD_CVaR    GLD_DD  GLD_RS_SPY  GLD_Corr_SPY  YC_Slope  \\\n",
            "Date                                                                 \n",
            "2025-02-27  0.057743  1.240611    1.591555      1.046644 -0.786289   \n",
            "2025-02-28  0.057743  1.196634    1.471813      1.117479 -0.790706   \n",
            "2025-03-03  0.057743  1.288561    1.651672      0.906123 -0.797578   \n",
            "2025-03-04  0.057743  1.350023    1.769134      0.827963 -0.802487   \n",
            "2025-03-05  0.057743  1.364859    1.641093      0.794538 -0.801996   \n",
            "\n",
            "            VIX_Regime   CPI_Mom  \n",
            "Date                              \n",
            "2025-02-27         0.0 -0.099737  \n",
            "2025-02-28         0.0 -0.099737  \n",
            "2025-03-03         0.0 -0.099737  \n",
            "2025-03-04         0.0 -0.099737  \n",
            "2025-03-05         0.0 -0.099737  \n",
            "\n",
            "[5 rows x 163 columns]\n",
            "\n",
            "Explained Variance Ratio from PCA:\n",
            "[0.20476065 0.13770648 0.10726689 0.07128199 0.06078197 0.04681147\n",
            " 0.0398866  0.03166858 0.02782046 0.02270486 0.01863823 0.01740729\n",
            " 0.01551778 0.0145725  0.01352644]\n"
          ]
        }
      ]
    }
  ]
}